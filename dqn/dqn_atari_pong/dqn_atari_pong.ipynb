{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple standalone dqn pong solution using target net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import collections\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gym wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FireResetEnv(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    press FIRE button if reqd and check for corner cases\n",
    "    \"\"\"\n",
    "    def __init__(self, env=None):\n",
    "        super(FireResetEnv, self).__init__(env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "        \n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(2)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    supports making an action decision every K frames\n",
    "    \"\"\"\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        self._obs_buffer = collections.deque(maxlen=2)\n",
    "        self._skip = skip\n",
    "        \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        uses same action for K frames and returns results with\n",
    "        the max valued frame of these frames\n",
    "        \"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "        \n",
    "    def reset(self):\n",
    "        self._obs_buffer.clear()\n",
    "        obs = self.env.reset()\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessFrame84(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    convert emulator observations from RGB 210x160 to grayscale 84x84\n",
    "    \"\"\"\n",
    "    def __init__(self, env=None):\n",
    "        super(ProcessFrame84, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0,\n",
    "                                               high=255,\n",
    "                                               shape=(84,84,1),\n",
    "                                               dtype=np.uint8)\n",
    "        \n",
    "    def observation(self, obs):\n",
    "        return ProcessFrame84.process(obs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def process(frame):\n",
    "        if frame.size == 210 * 160 * 3:\n",
    "            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
    "        elif frame.size == 250 * 160 * 3:\n",
    "            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
    "        else:\n",
    "            assert False, \"Unknown resolution\"\n",
    "            \n",
    "        img = img[:, :, 0]*0.299 + img[:, :, 1]*0.587 + img[:, :, 2]*0.114\n",
    "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
    "        x_t = resized_screen[18:102, :]\n",
    "        x_t = np.reshape(x_t, [84, 84, 1])\n",
    "        return x_t.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    keeps a stack of the 'n_steps' latest frames as the observation to return\n",
    "    \"\"\"\n",
    "    def __init__(self, env, n_steps, dtype=np.float32):\n",
    "        super(BufferWrapper, self).__init__(env)\n",
    "        self.dtype = dtype\n",
    "        self.observation_space = gym.spaces.Box(env.observation_space.low.repeat(n_steps, axis=0),\n",
    "                                               env.observation_space.high.repeat(n_steps, axis=0),\n",
    "                                                dtype=dtype)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
    "        return self.observation(self.env.reset())\n",
    "    \n",
    "    def observation(self, observation):\n",
    "        self.buffer[:-1] = self.buffer[1:]\n",
    "        self.buffer[-1] = observation\n",
    "        return self.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageToPytorch(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    move image channel to index 0 for pytorch compatibility\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPytorch, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0.0,\n",
    "                                               high=1.0,\n",
    "                                               shape=(self.observation_space.shape[-1],\n",
    "                                                     self.observation_space.shape[0],\n",
    "                                                     self.observation_space.shape[1]),\n",
    "                                                dtype=np.float32)\n",
    "        \n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    sets array type to float and normalizes pixel values\n",
    "    \"\"\"\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    env = MaxAndSkipEnv(env)\n",
    "    env = FireResetEnv(env)\n",
    "    env = ProcessFrame84(env)\n",
    "    env = ImageToPytorch(env)\n",
    "    env = BufferWrapper(env, 4)\n",
    "    return ScaledFloatFrame(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    based on DeepMinds architecture for\n",
    "    Human-Level Control through Deep Reinforcement Learning\n",
    "    published in Nature\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv = nn.Sequential(nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "                                 nn.ReLU())\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(nn.Linear(conv_out_size, 512),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(512, n_actions))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        conv -> transform 4d tensor to 2d tensor (where first dim in both is batch size) -> fc\n",
    "        \"\"\"\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)\n",
    "        \n",
    "    def _get_conv_out(self, shape):\n",
    "        \"\"\"\n",
    "        apply mock conv to get output size\n",
    "        \"\"\"\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "GYM_ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "MEAN_REWARD_BOUND = 19.0\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 32 # sampled from replay buffer\n",
    "\n",
    "REPLAY_MAX_SIZE = 10000\n",
    "REPLAY_START_SIZE = 10000 # num frames to populate replay buf before starting training\n",
    "SYNC_TARGET_FRAMES = 1000 # num frames btwn sync training and target models\n",
    "\n",
    "EPSILON_DECAY_LAST_FRAME = 150000\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward',\n",
    "                                                              'done', 'new_state'])\n",
    "\n",
    "class ExperienceBuffer:\n",
    "    \"\"\"\n",
    "    replay buffer of Experiences through games\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[i] for i in indices])\n",
    "        return (np.array(states), np.array(actions), np.array(rewards, dtype=np.float32),\n",
    "               np.array(dones, dtype=np.uint8), np.array(next_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, exp_buffer):\n",
    "        self.env = env\n",
    "        self.exp_buffer = exp_buffer\n",
    "        self._reset()\n",
    "        \n",
    "    def _reset(self):\n",
    "        self.state = env.reset()\n",
    "        self.total_reward = 0.0\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
    "        done_reward = None\n",
    "        \n",
    "        # select action\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_v = torch.tensor(np.array([self.state], copy=False)).to(device)\n",
    "            q_vals_v = net(state_v)\n",
    "            _, act_v = torch.max(q_vals_v, dim=1)\n",
    "            action = int(act_v.item())\n",
    "        \n",
    "        # play and save\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "        self.total_reward += reward\n",
    "        \n",
    "        exp = Experience(self.state, action, reward, is_done, new_state)\n",
    "        self.exp_buffer.append(exp)\n",
    "        self.state = new_state\n",
    "        if is_done:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "        return done_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(batch, net, tgt_net, device=\"cpu\"):\n",
    "    states, actions, rewards, dones, next_states = batch\n",
    "    states_v = torch.tensor(np.array(states, copy=False)).to(device)\n",
    "    next_states_v = torch.tensor(np.array(next_states, copy=False)).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.BoolTensor(dones).to(device)\n",
    "    \n",
    "    # run the sampled batch\n",
    "    # gather on dim 1 (actions)\n",
    "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        next_state_values = tgt_net(next_states_v).max(1)[0]\n",
    "        # no values for next state on last state\n",
    "        next_state_values[done_mask] = 0.0\n",
    "        # detach values from computation graph to prevent gradients\n",
    "        # from flowing into Q approx (of next state), (tgt) NN\n",
    "        next_state_values = next_state_values.detach()\n",
    "    expected_state_action_values = rewards_v + GAMMA*next_state_values\n",
    "    \n",
    "    return nn.MSELoss()(state_action_values, expected_state_action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "env = make_env(GYM_ENV_NAME)\n",
    "device = \"cpu\"\n",
    "\n",
    "# using a main NN instance for current state\n",
    "# and a target NN instance for next state\n",
    "# from the DeepMind paper\n",
    "net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "tgt_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "print(net)\n",
    "\n",
    "writer = SummaryWriter(comment=\"-dqn-\" + GYM_ENV_NAME)\n",
    "\n",
    "buffer = ExperienceBuffer(REPLAY_MAX_SIZE)\n",
    "agent = Agent(env, buffer)\n",
    "epsilon = EPSILON_START\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "total_rewards = []\n",
    "frame_i = 0\n",
    "ts_frame = 0\n",
    "ts = time.time()\n",
    "best_mean_reward = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "818: done 1 games, reward -21.000, eps 0.99, speed 188.56 fps\n",
      "1659: done 2 games, reward -20.500, eps 0.99, speed 1059.38 fps\n",
      "Best mean reward updated -21.000 -> -20.500\n",
      "2537: done 3 games, reward -20.667, eps 0.98, speed 1082.24 fps\n",
      "3466: done 4 games, reward -20.500, eps 0.98, speed 1083.76 fps\n",
      "4335: done 5 games, reward -20.400, eps 0.97, speed 1078.16 fps\n",
      "Best mean reward updated -20.500 -> -20.400\n",
      "5355: done 6 games, reward -20.333, eps 0.96, speed 1065.24 fps\n",
      "Best mean reward updated -20.400 -> -20.333\n",
      "6312: done 7 games, reward -20.429, eps 0.96, speed 1058.50 fps\n",
      "7093: done 8 games, reward -20.500, eps 0.95, speed 1061.02 fps\n",
      "7915: done 9 games, reward -20.556, eps 0.95, speed 1069.04 fps\n",
      "8982: done 10 games, reward -20.400, eps 0.94, speed 1064.86 fps\n",
      "10066: done 11 games, reward -20.273, eps 0.93, speed 474.20 fps\n",
      "Best mean reward updated -20.333 -> -20.273\n",
      "11080: done 12 games, reward -20.167, eps 0.93, speed 49.19 fps\n",
      "Best mean reward updated -20.273 -> -20.167\n",
      "11902: done 13 games, reward -20.231, eps 0.92, speed 49.19 fps\n",
      "12836: done 14 games, reward -20.286, eps 0.91, speed 45.10 fps\n",
      "13677: done 15 games, reward -20.333, eps 0.91, speed 47.71 fps\n",
      "14601: done 16 games, reward -20.312, eps 0.90, speed 47.07 fps\n",
      "15515: done 17 games, reward -20.294, eps 0.90, speed 48.24 fps\n",
      "16635: done 18 games, reward -20.167, eps 0.89, speed 47.77 fps\n",
      "17506: done 19 games, reward -20.211, eps 0.88, speed 47.81 fps\n",
      "18326: done 20 games, reward -20.250, eps 0.88, speed 48.56 fps\n",
      "19166: done 21 games, reward -20.238, eps 0.87, speed 49.35 fps\n",
      "19956: done 22 games, reward -20.273, eps 0.87, speed 49.66 fps\n",
      "20746: done 23 games, reward -20.304, eps 0.86, speed 48.23 fps\n",
      "21667: done 24 games, reward -20.292, eps 0.86, speed 49.01 fps\n",
      "22476: done 25 games, reward -20.320, eps 0.85, speed 48.71 fps\n",
      "23300: done 26 games, reward -20.346, eps 0.84, speed 48.91 fps\n",
      "24199: done 27 games, reward -20.333, eps 0.84, speed 50.15 fps\n",
      "25143: done 28 games, reward -20.357, eps 0.83, speed 50.04 fps\n",
      "26087: done 29 games, reward -20.379, eps 0.83, speed 49.99 fps\n",
      "26937: done 30 games, reward -20.400, eps 0.82, speed 49.68 fps\n",
      "27815: done 31 games, reward -20.419, eps 0.81, speed 49.96 fps\n",
      "28747: done 32 games, reward -20.406, eps 0.81, speed 49.63 fps\n",
      "29547: done 33 games, reward -20.424, eps 0.80, speed 49.63 fps\n",
      "30387: done 34 games, reward -20.412, eps 0.80, speed 48.39 fps\n",
      "31237: done 35 games, reward -20.429, eps 0.79, speed 49.69 fps\n",
      "32193: done 36 games, reward -20.417, eps 0.79, speed 49.71 fps\n",
      "33303: done 37 games, reward -20.405, eps 0.78, speed 57.18 fps\n",
      "34355: done 38 games, reward -20.395, eps 0.77, speed 55.76 fps\n",
      "35272: done 39 games, reward -20.385, eps 0.76, speed 55.03 fps\n",
      "36096: done 40 games, reward -20.400, eps 0.76, speed 54.74 fps\n",
      "37225: done 41 games, reward -20.341, eps 0.75, speed 55.93 fps\n",
      "38374: done 42 games, reward -20.310, eps 0.74, speed 55.51 fps\n",
      "39270: done 43 games, reward -20.302, eps 0.74, speed 55.22 fps\n",
      "40492: done 44 games, reward -20.295, eps 0.73, speed 55.19 fps\n",
      "41424: done 45 games, reward -20.311, eps 0.72, speed 55.32 fps\n",
      "42474: done 46 games, reward -20.304, eps 0.72, speed 55.19 fps\n",
      "43508: done 47 games, reward -20.319, eps 0.71, speed 55.46 fps\n",
      "44705: done 48 games, reward -20.271, eps 0.70, speed 55.40 fps\n",
      "45765: done 49 games, reward -20.245, eps 0.69, speed 55.03 fps\n",
      "46676: done 50 games, reward -20.260, eps 0.69, speed 54.94 fps\n",
      "47554: done 51 games, reward -20.275, eps 0.68, speed 54.82 fps\n",
      "48376: done 52 games, reward -20.288, eps 0.68, speed 55.20 fps\n",
      "49410: done 53 games, reward -20.302, eps 0.67, speed 55.02 fps\n",
      "50610: done 54 games, reward -20.315, eps 0.66, speed 55.07 fps\n",
      "51568: done 55 games, reward -20.327, eps 0.66, speed 55.54 fps\n",
      "52637: done 56 games, reward -20.304, eps 0.65, speed 54.81 fps\n",
      "54016: done 57 games, reward -20.211, eps 0.64, speed 54.69 fps\n",
      "55005: done 58 games, reward -20.224, eps 0.63, speed 54.09 fps\n",
      "56274: done 59 games, reward -20.169, eps 0.62, speed 53.74 fps\n",
      "57344: done 60 games, reward -20.150, eps 0.62, speed 54.45 fps\n",
      "Best mean reward updated -20.167 -> -20.150\n",
      "58629: done 61 games, reward -20.115, eps 0.61, speed 55.25 fps\n",
      "Best mean reward updated -20.150 -> -20.115\n",
      "59773: done 62 games, reward -20.113, eps 0.60, speed 55.21 fps\n",
      "Best mean reward updated -20.115 -> -20.113\n",
      "60997: done 63 games, reward -20.127, eps 0.59, speed 53.50 fps\n",
      "61967: done 64 games, reward -20.125, eps 0.59, speed 54.99 fps\n",
      "63407: done 65 games, reward -20.077, eps 0.58, speed 52.69 fps\n",
      "Best mean reward updated -20.113 -> -20.077\n",
      "64713: done 66 games, reward -20.045, eps 0.57, speed 54.00 fps\n",
      "Best mean reward updated -20.077 -> -20.045\n",
      "65940: done 67 games, reward -20.000, eps 0.56, speed 54.37 fps\n",
      "Best mean reward updated -20.045 -> -20.000\n",
      "67708: done 68 games, reward -19.956, eps 0.55, speed 54.51 fps\n",
      "Best mean reward updated -20.000 -> -19.956\n",
      "68921: done 69 games, reward -19.957, eps 0.54, speed 54.40 fps\n",
      "70320: done 70 games, reward -19.957, eps 0.53, speed 53.68 fps\n",
      "71747: done 71 games, reward -19.958, eps 0.52, speed 52.68 fps\n",
      "73384: done 72 games, reward -19.889, eps 0.51, speed 50.99 fps\n",
      "Best mean reward updated -19.956 -> -19.889\n",
      "74689: done 73 games, reward -19.904, eps 0.50, speed 50.01 fps\n",
      "76245: done 74 games, reward -19.865, eps 0.49, speed 49.06 fps\n",
      "Best mean reward updated -19.889 -> -19.865\n",
      "77498: done 75 games, reward -19.867, eps 0.48, speed 48.76 fps\n",
      "78840: done 76 games, reward -19.842, eps 0.47, speed 48.34 fps\n",
      "Best mean reward updated -19.865 -> -19.842\n",
      "80057: done 77 games, reward -19.844, eps 0.47, speed 48.06 fps\n",
      "81350: done 78 games, reward -19.846, eps 0.46, speed 48.05 fps\n",
      "82553: done 79 games, reward -19.861, eps 0.45, speed 47.61 fps\n",
      "83781: done 80 games, reward -19.850, eps 0.44, speed 47.08 fps\n",
      "85110: done 81 games, reward -19.840, eps 0.43, speed 47.32 fps\n",
      "Best mean reward updated -19.842 -> -19.840\n",
      "86678: done 82 games, reward -19.841, eps 0.42, speed 47.03 fps\n",
      "88336: done 83 games, reward -19.843, eps 0.41, speed 47.53 fps\n",
      "89519: done 84 games, reward -19.833, eps 0.40, speed 47.56 fps\n",
      "Best mean reward updated -19.840 -> -19.833\n",
      "90666: done 85 games, reward -19.824, eps 0.40, speed 47.51 fps\n",
      "Best mean reward updated -19.833 -> -19.824\n",
      "91861: done 86 games, reward -19.826, eps 0.39, speed 47.23 fps\n",
      "93224: done 87 games, reward -19.805, eps 0.38, speed 46.88 fps\n",
      "Best mean reward updated -19.824 -> -19.805\n",
      "94432: done 88 games, reward -19.807, eps 0.37, speed 45.17 fps\n",
      "96559: done 89 games, reward -19.719, eps 0.36, speed 47.16 fps\n",
      "Best mean reward updated -19.805 -> -19.719\n",
      "98127: done 90 games, reward -19.711, eps 0.35, speed 46.56 fps\n",
      "Best mean reward updated -19.719 -> -19.711\n",
      "99634: done 91 games, reward -19.681, eps 0.34, speed 47.09 fps\n",
      "Best mean reward updated -19.711 -> -19.681\n",
      "101062: done 92 games, reward -19.652, eps 0.33, speed 46.38 fps\n",
      "Best mean reward updated -19.681 -> -19.652\n",
      "102941: done 93 games, reward -19.591, eps 0.31, speed 46.21 fps\n",
      "Best mean reward updated -19.652 -> -19.591\n",
      "104520: done 94 games, reward -19.596, eps 0.30, speed 45.69 fps\n",
      "105768: done 95 games, reward -19.589, eps 0.29, speed 45.08 fps\n",
      "Best mean reward updated -19.591 -> -19.589\n",
      "107515: done 96 games, reward -19.562, eps 0.28, speed 42.61 fps\n",
      "Best mean reward updated -19.589 -> -19.562\n",
      "108940: done 97 games, reward -19.546, eps 0.27, speed 36.02 fps\n",
      "Best mean reward updated -19.562 -> -19.546\n",
      "110738: done 98 games, reward -19.500, eps 0.26, speed 37.78 fps\n",
      "Best mean reward updated -19.546 -> -19.500\n",
      "112117: done 99 games, reward -19.475, eps 0.25, speed 46.37 fps\n",
      "Best mean reward updated -19.500 -> -19.475\n",
      "114039: done 100 games, reward -19.440, eps 0.24, speed 46.36 fps\n",
      "Best mean reward updated -19.475 -> -19.440\n",
      "116163: done 101 games, reward -19.400, eps 0.23, speed 42.56 fps\n",
      "Best mean reward updated -19.440 -> -19.400\n",
      "118334: done 102 games, reward -19.330, eps 0.21, speed 43.31 fps\n",
      "Best mean reward updated -19.400 -> -19.330\n",
      "119729: done 103 games, reward -19.310, eps 0.20, speed 46.31 fps\n",
      "Best mean reward updated -19.330 -> -19.310\n",
      "122034: done 104 games, reward -19.190, eps 0.19, speed 45.85 fps\n",
      "Best mean reward updated -19.310 -> -19.190\n",
      "123925: done 105 games, reward -19.120, eps 0.17, speed 43.67 fps\n",
      "Best mean reward updated -19.190 -> -19.120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126269: done 106 games, reward -19.040, eps 0.16, speed 45.78 fps\n",
      "Best mean reward updated -19.120 -> -19.040\n",
      "128394: done 107 games, reward -18.970, eps 0.14, speed 46.17 fps\n",
      "Best mean reward updated -19.040 -> -18.970\n",
      "131136: done 108 games, reward -18.790, eps 0.13, speed 46.58 fps\n",
      "Best mean reward updated -18.970 -> -18.790\n",
      "133102: done 109 games, reward -18.720, eps 0.11, speed 46.56 fps\n",
      "Best mean reward updated -18.790 -> -18.720\n",
      "136426: done 110 games, reward -18.580, eps 0.09, speed 46.50 fps\n",
      "Best mean reward updated -18.720 -> -18.580\n",
      "139436: done 111 games, reward -18.460, eps 0.07, speed 46.10 fps\n",
      "Best mean reward updated -18.580 -> -18.460\n",
      "143245: done 112 games, reward -18.250, eps 0.05, speed 45.80 fps\n",
      "Best mean reward updated -18.460 -> -18.250\n",
      "146601: done 113 games, reward -18.030, eps 0.02, speed 45.84 fps\n",
      "Best mean reward updated -18.250 -> -18.030\n",
      "148611: done 114 games, reward -17.650, eps 0.01, speed 45.30 fps\n",
      "Best mean reward updated -18.030 -> -17.650\n",
      "150837: done 115 games, reward -17.280, eps 0.01, speed 45.73 fps\n",
      "Best mean reward updated -17.650 -> -17.280\n",
      "152765: done 116 games, reward -16.900, eps 0.01, speed 46.04 fps\n",
      "Best mean reward updated -17.280 -> -16.900\n",
      "154805: done 117 games, reward -16.540, eps 0.01, speed 45.79 fps\n",
      "Best mean reward updated -16.900 -> -16.540\n",
      "156902: done 118 games, reward -16.170, eps 0.01, speed 45.30 fps\n",
      "Best mean reward updated -16.540 -> -16.170\n",
      "158798: done 119 games, reward -15.780, eps 0.01, speed 45.40 fps\n",
      "Best mean reward updated -16.170 -> -15.780\n",
      "160469: done 120 games, reward -15.370, eps 0.01, speed 45.72 fps\n",
      "Best mean reward updated -15.780 -> -15.370\n",
      "163228: done 121 games, reward -15.080, eps 0.01, speed 45.85 fps\n",
      "Best mean reward updated -15.370 -> -15.080\n",
      "165228: done 122 games, reward -14.700, eps 0.01, speed 46.20 fps\n",
      "Best mean reward updated -15.080 -> -14.700\n",
      "167069: done 123 games, reward -14.300, eps 0.01, speed 45.79 fps\n",
      "Best mean reward updated -14.700 -> -14.300\n",
      "169114: done 124 games, reward -13.930, eps 0.01, speed 45.91 fps\n",
      "Best mean reward updated -14.300 -> -13.930\n",
      "170832: done 125 games, reward -13.520, eps 0.01, speed 45.64 fps\n",
      "Best mean reward updated -13.930 -> -13.520\n",
      "173947: done 126 games, reward -13.300, eps 0.01, speed 45.28 fps\n",
      "Best mean reward updated -13.520 -> -13.300\n",
      "176140: done 127 games, reward -12.950, eps 0.01, speed 45.57 fps\n",
      "Best mean reward updated -13.300 -> -12.950\n",
      "177906: done 128 games, reward -12.550, eps 0.01, speed 45.43 fps\n",
      "Best mean reward updated -12.950 -> -12.550\n",
      "179577: done 129 games, reward -12.140, eps 0.01, speed 45.99 fps\n",
      "Best mean reward updated -12.550 -> -12.140\n",
      "181443: done 130 games, reward -11.750, eps 0.01, speed 46.21 fps\n",
      "Best mean reward updated -12.140 -> -11.750\n",
      "183115: done 131 games, reward -11.340, eps 0.01, speed 45.58 fps\n",
      "Best mean reward updated -11.750 -> -11.340\n",
      "184832: done 132 games, reward -10.940, eps 0.01, speed 45.73 fps\n",
      "Best mean reward updated -11.340 -> -10.940\n",
      "187214: done 133 games, reward -10.640, eps 0.01, speed 45.67 fps\n",
      "Best mean reward updated -10.940 -> -10.640\n",
      "188999: done 134 games, reward -10.250, eps 0.01, speed 45.71 fps\n",
      "Best mean reward updated -10.640 -> -10.250\n",
      "190831: done 135 games, reward -9.850, eps 0.01, speed 45.92 fps\n",
      "Best mean reward updated -10.250 -> -9.850\n",
      "192741: done 136 games, reward -9.460, eps 0.01, speed 45.83 fps\n",
      "Best mean reward updated -9.850 -> -9.460\n",
      "194501: done 137 games, reward -9.060, eps 0.01, speed 46.21 fps\n",
      "Best mean reward updated -9.460 -> -9.060\n",
      "196849: done 138 games, reward -8.730, eps 0.01, speed 45.81 fps\n",
      "Best mean reward updated -9.060 -> -8.730\n",
      "198826: done 139 games, reward -8.370, eps 0.01, speed 45.36 fps\n",
      "Best mean reward updated -8.730 -> -8.370\n",
      "200661: done 140 games, reward -7.970, eps 0.01, speed 45.81 fps\n",
      "Best mean reward updated -8.370 -> -7.970\n",
      "202606: done 141 games, reward -7.620, eps 0.01, speed 45.82 fps\n",
      "Best mean reward updated -7.970 -> -7.620\n",
      "204400: done 142 games, reward -7.230, eps 0.01, speed 45.88 fps\n",
      "Best mean reward updated -7.620 -> -7.230\n",
      "206386: done 143 games, reward -6.860, eps 0.01, speed 45.54 fps\n",
      "Best mean reward updated -7.230 -> -6.860\n",
      "208110: done 144 games, reward -6.470, eps 0.01, speed 45.27 fps\n",
      "Best mean reward updated -6.860 -> -6.470\n",
      "209747: done 145 games, reward -6.050, eps 0.01, speed 45.58 fps\n",
      "Best mean reward updated -6.470 -> -6.050\n",
      "211530: done 146 games, reward -5.660, eps 0.01, speed 45.64 fps\n",
      "Best mean reward updated -6.050 -> -5.660\n",
      "213390: done 147 games, reward -5.280, eps 0.01, speed 46.13 fps\n",
      "Best mean reward updated -5.660 -> -5.280\n",
      "215634: done 148 games, reward -4.960, eps 0.01, speed 45.97 fps\n",
      "Best mean reward updated -5.280 -> -4.960\n",
      "217513: done 149 games, reward -4.590, eps 0.01, speed 45.85 fps\n",
      "Best mean reward updated -4.960 -> -4.590\n",
      "220263: done 150 games, reward -4.360, eps 0.01, speed 45.78 fps\n",
      "Best mean reward updated -4.590 -> -4.360\n",
      "222292: done 151 games, reward -4.000, eps 0.01, speed 45.72 fps\n",
      "Best mean reward updated -4.360 -> -4.000\n",
      "224131: done 152 games, reward -3.600, eps 0.01, speed 45.61 fps\n",
      "Best mean reward updated -4.000 -> -3.600\n",
      "226401: done 153 games, reward -3.280, eps 0.01, speed 45.50 fps\n",
      "Best mean reward updated -3.600 -> -3.280\n",
      "228279: done 154 games, reward -2.890, eps 0.01, speed 45.59 fps\n",
      "Best mean reward updated -3.280 -> -2.890\n",
      "229978: done 155 games, reward -2.480, eps 0.01, speed 45.93 fps\n",
      "Best mean reward updated -2.890 -> -2.480\n",
      "231649: done 156 games, reward -2.090, eps 0.01, speed 45.78 fps\n",
      "Best mean reward updated -2.480 -> -2.090\n",
      "233286: done 157 games, reward -1.730, eps 0.01, speed 45.93 fps\n",
      "Best mean reward updated -2.090 -> -1.730\n",
      "235021: done 158 games, reward -1.320, eps 0.01, speed 46.08 fps\n",
      "Best mean reward updated -1.730 -> -1.320\n",
      "236958: done 159 games, reward -0.980, eps 0.01, speed 45.56 fps\n",
      "Best mean reward updated -1.320 -> -0.980\n",
      "238896: done 160 games, reward -0.600, eps 0.01, speed 45.82 fps\n",
      "Best mean reward updated -0.980 -> -0.600\n",
      "241136: done 161 games, reward -0.280, eps 0.01, speed 45.59 fps\n",
      "Best mean reward updated -0.600 -> -0.280\n",
      "242968: done 162 games, reward 0.110, eps 0.01, speed 45.68 fps\n",
      "Best mean reward updated -0.280 -> 0.110\n",
      "244653: done 163 games, reward 0.520, eps 0.01, speed 46.25 fps\n",
      "Best mean reward updated 0.110 -> 0.520\n",
      "246507: done 164 games, reward 0.900, eps 0.01, speed 46.21 fps\n",
      "Best mean reward updated 0.520 -> 0.900\n",
      "248178: done 165 games, reward 1.270, eps 0.01, speed 46.12 fps\n",
      "Best mean reward updated 0.900 -> 1.270\n",
      "249813: done 166 games, reward 1.660, eps 0.01, speed 45.87 fps\n",
      "Best mean reward updated 1.270 -> 1.660\n",
      "251545: done 167 games, reward 2.020, eps 0.01, speed 46.04 fps\n",
      "Best mean reward updated 1.660 -> 2.020\n",
      "253332: done 168 games, reward 2.380, eps 0.01, speed 45.76 fps\n",
      "Best mean reward updated 2.020 -> 2.380\n",
      "255120: done 169 games, reward 2.760, eps 0.01, speed 45.58 fps\n",
      "Best mean reward updated 2.380 -> 2.760\n",
      "256978: done 170 games, reward 3.140, eps 0.01, speed 45.68 fps\n",
      "Best mean reward updated 2.760 -> 3.140\n",
      "258615: done 171 games, reward 3.550, eps 0.01, speed 45.88 fps\n",
      "Best mean reward updated 3.140 -> 3.550\n",
      "260404: done 172 games, reward 3.890, eps 0.01, speed 45.88 fps\n",
      "Best mean reward updated 3.550 -> 3.890\n",
      "262043: done 173 games, reward 4.310, eps 0.01, speed 45.64 fps\n",
      "Best mean reward updated 3.890 -> 4.310\n",
      "263678: done 174 games, reward 4.690, eps 0.01, speed 46.09 fps\n",
      "Best mean reward updated 4.310 -> 4.690\n",
      "265443: done 175 games, reward 5.080, eps 0.01, speed 45.85 fps\n",
      "Best mean reward updated 4.690 -> 5.080\n",
      "267269: done 176 games, reward 5.450, eps 0.01, speed 45.62 fps\n",
      "Best mean reward updated 5.080 -> 5.450\n",
      "268999: done 177 games, reward 5.850, eps 0.01, speed 46.31 fps\n",
      "Best mean reward updated 5.450 -> 5.850\n",
      "270634: done 178 games, reward 6.260, eps 0.01, speed 45.86 fps\n",
      "Best mean reward updated 5.850 -> 6.260\n",
      "272394: done 179 games, reward 6.660, eps 0.01, speed 45.81 fps\n",
      "Best mean reward updated 6.260 -> 6.660\n",
      "274030: done 180 games, reward 7.060, eps 0.01, speed 45.36 fps\n",
      "Best mean reward updated 6.660 -> 7.060\n",
      "275666: done 181 games, reward 7.460, eps 0.01, speed 45.36 fps\n",
      "Best mean reward updated 7.060 -> 7.460\n",
      "277340: done 182 games, reward 7.860, eps 0.01, speed 45.89 fps\n",
      "Best mean reward updated 7.460 -> 7.860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279062: done 183 games, reward 8.250, eps 0.01, speed 45.89 fps\n",
      "Best mean reward updated 7.860 -> 8.250\n",
      "280749: done 184 games, reward 8.640, eps 0.01, speed 45.58 fps\n",
      "Best mean reward updated 8.250 -> 8.640\n",
      "282539: done 185 games, reward 9.020, eps 0.01, speed 46.12 fps\n",
      "Best mean reward updated 8.640 -> 9.020\n",
      "284211: done 186 games, reward 9.420, eps 0.01, speed 45.85 fps\n",
      "Best mean reward updated 9.020 -> 9.420\n",
      "285912: done 187 games, reward 9.800, eps 0.01, speed 46.21 fps\n",
      "Best mean reward updated 9.420 -> 9.800\n",
      "287583: done 188 games, reward 10.200, eps 0.01, speed 45.55 fps\n",
      "Best mean reward updated 9.800 -> 10.200\n",
      "289218: done 189 games, reward 10.530, eps 0.01, speed 46.00 fps\n",
      "Best mean reward updated 10.200 -> 10.530\n",
      "290853: done 190 games, reward 10.930, eps 0.01, speed 45.94 fps\n",
      "Best mean reward updated 10.530 -> 10.930\n",
      "292488: done 191 games, reward 11.310, eps 0.01, speed 45.87 fps\n",
      "Best mean reward updated 10.930 -> 11.310\n",
      "294123: done 192 games, reward 11.690, eps 0.01, speed 45.83 fps\n",
      "Best mean reward updated 11.310 -> 11.690\n",
      "296383: done 193 games, reward 11.870, eps 0.01, speed 45.85 fps\n",
      "Best mean reward updated 11.690 -> 11.870\n",
      "298018: done 194 games, reward 12.280, eps 0.01, speed 45.89 fps\n",
      "Best mean reward updated 11.870 -> 12.280\n",
      "299653: done 195 games, reward 12.680, eps 0.01, speed 46.05 fps\n",
      "Best mean reward updated 12.280 -> 12.680\n",
      "301289: done 196 games, reward 13.060, eps 0.01, speed 46.12 fps\n",
      "Best mean reward updated 12.680 -> 13.060\n",
      "302925: done 197 games, reward 13.450, eps 0.01, speed 46.04 fps\n",
      "Best mean reward updated 13.060 -> 13.450\n",
      "305016: done 198 games, reward 13.730, eps 0.01, speed 46.11 fps\n",
      "Best mean reward updated 13.450 -> 13.730\n",
      "306767: done 199 games, reward 14.090, eps 0.01, speed 45.92 fps\n",
      "Best mean reward updated 13.730 -> 14.090\n",
      "308403: done 200 games, reward 14.460, eps 0.01, speed 46.25 fps\n",
      "Best mean reward updated 14.090 -> 14.460\n",
      "310194: done 201 games, reward 14.830, eps 0.01, speed 46.04 fps\n",
      "Best mean reward updated 14.460 -> 14.830\n",
      "311860: done 202 games, reward 15.160, eps 0.01, speed 45.79 fps\n",
      "Best mean reward updated 14.830 -> 15.160\n",
      "313677: done 203 games, reward 15.540, eps 0.01, speed 45.59 fps\n",
      "Best mean reward updated 15.160 -> 15.540\n",
      "315555: done 204 games, reward 15.800, eps 0.01, speed 45.62 fps\n",
      "Best mean reward updated 15.540 -> 15.800\n",
      "317693: done 205 games, reward 16.080, eps 0.01, speed 45.84 fps\n",
      "Best mean reward updated 15.800 -> 16.080\n",
      "319693: done 206 games, reward 16.370, eps 0.01, speed 45.80 fps\n",
      "Best mean reward updated 16.080 -> 16.370\n",
      "321633: done 207 games, reward 16.690, eps 0.01, speed 46.03 fps\n",
      "Best mean reward updated 16.370 -> 16.690\n",
      "323989: done 208 games, reward 16.860, eps 0.01, speed 46.44 fps\n",
      "Best mean reward updated 16.690 -> 16.860\n",
      "325801: done 209 games, reward 17.180, eps 0.01, speed 46.07 fps\n",
      "Best mean reward updated 16.860 -> 17.180\n",
      "327652: done 210 games, reward 17.420, eps 0.01, speed 46.19 fps\n",
      "Best mean reward updated 17.180 -> 17.420\n",
      "329707: done 211 games, reward 17.670, eps 0.01, speed 46.72 fps\n",
      "Best mean reward updated 17.420 -> 17.670\n",
      "331867: done 212 games, reward 17.810, eps 0.01, speed 45.92 fps\n",
      "Best mean reward updated 17.670 -> 17.810\n",
      "333747: done 213 games, reward 17.990, eps 0.01, speed 46.29 fps\n",
      "Best mean reward updated 17.810 -> 17.990\n",
      "335542: done 214 games, reward 18.010, eps 0.01, speed 46.21 fps\n",
      "Best mean reward updated 17.990 -> 18.010\n",
      "337288: done 215 games, reward 18.040, eps 0.01, speed 46.49 fps\n",
      "Best mean reward updated 18.010 -> 18.040\n",
      "339082: done 216 games, reward 18.050, eps 0.01, speed 46.18 fps\n",
      "Best mean reward updated 18.040 -> 18.050\n",
      "341038: done 217 games, reward 18.050, eps 0.01, speed 46.18 fps\n",
      "342881: done 218 games, reward 18.040, eps 0.01, speed 46.44 fps\n",
      "344762: done 219 games, reward 18.050, eps 0.01, speed 46.09 fps\n",
      "346648: done 220 games, reward 18.030, eps 0.01, speed 46.42 fps\n",
      "348283: done 221 games, reward 18.150, eps 0.01, speed 46.28 fps\n",
      "Best mean reward updated 18.050 -> 18.150\n",
      "350168: done 222 games, reward 18.160, eps 0.01, speed 46.19 fps\n",
      "Best mean reward updated 18.150 -> 18.160\n",
      "352364: done 223 games, reward 18.130, eps 0.01, speed 45.95 fps\n",
      "354845: done 224 games, reward 18.090, eps 0.01, speed 46.19 fps\n",
      "356780: done 225 games, reward 18.080, eps 0.01, speed 46.26 fps\n",
      "358601: done 226 games, reward 18.250, eps 0.01, speed 46.49 fps\n",
      "Best mean reward updated 18.160 -> 18.250\n",
      "360237: done 227 games, reward 18.310, eps 0.01, speed 46.41 fps\n",
      "Best mean reward updated 18.250 -> 18.310\n",
      "362196: done 228 games, reward 18.310, eps 0.01, speed 46.16 fps\n",
      "364014: done 229 games, reward 18.310, eps 0.01, speed 46.41 fps\n",
      "365758: done 230 games, reward 18.330, eps 0.01, speed 46.32 fps\n",
      "Best mean reward updated 18.310 -> 18.330\n",
      "367444: done 231 games, reward 18.330, eps 0.01, speed 45.72 fps\n",
      "369078: done 232 games, reward 18.340, eps 0.01, speed 46.20 fps\n",
      "Best mean reward updated 18.330 -> 18.340\n",
      "370994: done 233 games, reward 18.430, eps 0.01, speed 46.50 fps\n",
      "Best mean reward updated 18.340 -> 18.430\n",
      "372629: done 234 games, reward 18.450, eps 0.01, speed 45.67 fps\n",
      "Best mean reward updated 18.430 -> 18.450\n",
      "374264: done 235 games, reward 18.470, eps 0.01, speed 46.03 fps\n",
      "Best mean reward updated 18.450 -> 18.470\n",
      "376054: done 236 games, reward 18.460, eps 0.01, speed 46.47 fps\n",
      "377989: done 237 games, reward 18.420, eps 0.01, speed 45.70 fps\n",
      "379624: done 238 games, reward 18.500, eps 0.01, speed 45.87 fps\n",
      "Best mean reward updated 18.470 -> 18.500\n",
      "381340: done 239 games, reward 18.530, eps 0.01, speed 46.22 fps\n",
      "Best mean reward updated 18.500 -> 18.530\n",
      "383098: done 240 games, reward 18.530, eps 0.01, speed 46.04 fps\n",
      "384797: done 241 games, reward 18.560, eps 0.01, speed 45.87 fps\n",
      "Best mean reward updated 18.530 -> 18.560\n",
      "386432: done 242 games, reward 18.570, eps 0.01, speed 46.14 fps\n",
      "Best mean reward updated 18.560 -> 18.570\n",
      "388066: done 243 games, reward 18.610, eps 0.01, speed 45.85 fps\n",
      "Best mean reward updated 18.570 -> 18.610\n",
      "389757: done 244 games, reward 18.620, eps 0.01, speed 46.32 fps\n",
      "Best mean reward updated 18.610 -> 18.620\n",
      "391549: done 245 games, reward 18.590, eps 0.01, speed 45.66 fps\n",
      "393200: done 246 games, reward 18.610, eps 0.01, speed 46.85 fps\n",
      "395045: done 247 games, reward 18.640, eps 0.01, speed 46.38 fps\n",
      "Best mean reward updated 18.620 -> 18.640\n",
      "396677: done 248 games, reward 18.710, eps 0.01, speed 46.46 fps\n",
      "Best mean reward updated 18.640 -> 18.710\n",
      "398636: done 249 games, reward 18.720, eps 0.01, speed 46.09 fps\n",
      "Best mean reward updated 18.710 -> 18.720\n",
      "400610: done 250 games, reward 18.860, eps 0.01, speed 45.87 fps\n",
      "Best mean reward updated 18.720 -> 18.860\n",
      "402246: done 251 games, reward 18.920, eps 0.01, speed 46.07 fps\n",
      "Best mean reward updated 18.860 -> 18.920\n",
      "403880: done 252 games, reward 18.940, eps 0.01, speed 46.39 fps\n",
      "Best mean reward updated 18.920 -> 18.940\n",
      "405515: done 253 games, reward 19.040, eps 0.01, speed 46.40 fps\n",
      "Best mean reward updated 18.940 -> 19.040\n",
      "Solved in 405515 frames\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "while True:\n",
    "    frame_i +=1\n",
    "    epsilon = max(EPSILON_END, EPSILON_START - frame_i / EPSILON_DECAY_LAST_FRAME)\n",
    "    \n",
    "    reward = agent.play_step(net, epsilon, device=device)\n",
    "    if reward is not None:\n",
    "        \"\"\"\n",
    "        it was the last step in an epsiode\n",
    "        \"\"\"\n",
    "        total_rewards.append(reward)\n",
    "        speed = (frame_i - ts_frame) / (time.time() - ts)\n",
    "        ts_frame = frame_i\n",
    "        ts = time.time()\n",
    "        mean_reward = np.mean(total_rewards[-100:])\n",
    "        print(\"%d: done %d games, reward %.3f, eps %.2f, speed %.2f fps\" % (frame_i, len(total_rewards),\n",
    "                                          mean_reward, epsilon, speed))\n",
    "        writer.add_scalar(\"reward_mean_100_frames\", mean_reward, frame_i)\n",
    "        writer.add_scalar(\"reward_latest\", reward, frame_i)\n",
    "        writer.add_scalar(\"epsilon\", epsilon, frame_i)\n",
    "        writer.add_scalar(\"speed\", speed, frame_i)\n",
    "        \n",
    "        if best_mean_reward is None or best_mean_reward < mean_reward:\n",
    "            # save model params\n",
    "            torch.save(net.state_dict(), \"models/\" + GYM_ENV_NAME + \"-best_%.0f.dat\" % mean_reward)\n",
    "            if best_mean_reward is not None:\n",
    "                print(\"Best mean reward updated %.3f -> %.3f\" % (best_mean_reward, mean_reward))\n",
    "            best_mean_reward = mean_reward\n",
    "        if mean_reward > MEAN_REWARD_BOUND:\n",
    "            print(\"Solved in %d frames\" % frame_i)\n",
    "            break\n",
    "    \n",
    "    # enough transitions in buffer to start training?\n",
    "    if len(buffer) < REPLAY_START_SIZE:\n",
    "        continue\n",
    "    \n",
    "    # sync models\n",
    "    if frame_i % SYNC_TARGET_FRAMES == 0:\n",
    "        tgt_net.load_state_dict(net.state_dict())\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    batch = buffer.sample(BATCH_SIZE)\n",
    "    loss_t = calc_loss(batch, net, tgt_net, device=device)\n",
    "    loss_t.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### play with models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FILE = \"models/\" + GYM_ENV_NAME + \"-best_19.dat\"\n",
    "FPS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(GYM_ENV_NAME)\n",
    "#env = gym.wrappers.Monitor(env, directory='monitor', force=True)\n",
    "\n",
    "net = DQN(env.observation_space.shape, env.action_space.n)\n",
    "state = torch.load(MODEL_FILE, map_location=lambda stg,_: stg)\n",
    "net.load_state_dict(state)\n",
    "\n",
    "state = env.reset()\n",
    "total_reward = 0.0\n",
    "c = collections.Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 20.00\n",
      "Action counts: Counter({0: 616, 3: 362, 4: 203, 1: 198, 2: 163, 5: 152})\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    start_ts = time.time()\n",
    "    env.render()\n",
    "    \n",
    "    # select action with model\n",
    "    state_v = torch.tensor(np.array([state], copy=False))\n",
    "    q_vals = net(state_v).data.numpy()[0]\n",
    "    action = np.argmax(q_vals)\n",
    "    c[action] += 1\n",
    "    \n",
    "    # play\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    if done:\n",
    "        break\n",
    "    \n",
    "    delta = 1/FPS - (time.time() - start_ts)\n",
    "    if delta > 0:\n",
    "        time.sleep(delta)\n",
    "    \n",
    "print(\"Total reward: %.2f\" % total_reward)\n",
    "print(\"Action counts:\", c)\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
